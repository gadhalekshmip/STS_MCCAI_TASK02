
#from google.colab import drive
#drive.mount('/content/drive')


# Core libraries
#!pip install open3d pycpd scikit-learn pandas tqdm scipy matplotlib
#!pip install SimpleITK

"""
*   **Chamfer Distance:** A function for calculating the Chamfer distance between two point clouds, used as a loss function during training.
*   **PointNet Architecture:** Definitions for the PointNet feature extractor and related components.
*   **PointNetLK Module:** Implementation of the PointNetLK registration algorithm.
*   **PointNetLKUnifiedModel:** A unified model combining PointNet feature extraction and PointNetLK registration.
*   **Dataset Classes:** Custom PyTorch `Dataset` classes (`UnlabeledPointCloudDataset` and `LabeledPointCloudDataset`) for loading and preprocessing point cloud data, including normalization and outlier removal.
*   **Visualization Function:** A utility function for visualizing registration results.
*   **Post-processing Function:** An implementation of ICP-based post-processing (although its full application is limited by available validation data).
*   **Training and Prediction Functions:** Helper functions (`run_training` and `generate_predictions`) to streamline the training and inference workflows.


"""

import torch

# Chamfer Distance
def chamfer_distance(p1, p2):
    x, y = p1, p2
    x_exp = x.unsqueeze(2)
    y_exp = y.unsqueeze(1)
    dist = torch.sum((x_exp - y_exp) ** 2, dim=-1)
    dist_x_to_y = torch.min(dist, dim=2)[0]
    dist_y_to_x = torch.min(dist, dim=1)[0]
    return dist_x_to_y.mean(dim=1) + dist_y_to_x.mean(dim=1)



import torch.nn as nn
import torch.nn.functional as F
import torch

class FeatureTransformNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = torch.nn.Conv1d(64, 64, 1)
        self.conv2 = torch.nn.Conv1d(64, 128, 1)
        self.conv3 = torch.nn.Conv1d(128, 1024, 1)
        self.bn1 = nn.BatchNorm1d(64)
        self.bn2 = nn.BatchNorm1d(128)
        self.bn3 = nn.BatchNorm1d(1024)
        self.fc1 = nn.Linear(1024, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, 64 * 64)
        self.bn4 = nn.BatchNorm1d(512)
        self.bn5 = nn.BatchNorm1d(256)

    def forward(self, x):
        B = x.size(0)
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.relu(self.bn3(self.conv3(x)))
        x = torch.max(x, 2, keepdim=True)[0]
        x = x.view(-1, 1024)

        x = F.relu(self.bn4(self.fc1(x)))
        x = F.relu(self.bn5(self.fc2(x)))
        x = self.fc3(x)

        # Ensure identity matrix is on the correct device
        iden = torch.eye(64, requires_grad=True).repeat(B, 1, 1).to(x.device)
        x = x.view(-1, 64, 64) + iden
        return x


import torch.nn as nn
import torch.nn.functional as F
import torch

class PointNetFeatureExtractor(nn.Module):
    def __init__(self, global_feat=True, feature_transform=False):
        super().__init__()
        self.conv1 = torch.nn.Conv1d(3, 64, 1)
        self.conv2 = torch.nn.Conv1d(64, 128, 1)
        self.conv3 = torch.nn.Conv1d(128, 1024, 1)
        self.bn1 = nn.BatchNorm1d(64)
        self.bn2 = nn.BatchNorm1d(128)
        self.bn3 = nn.BatchNorm1d(1024)
        self.global_feat = global_feat
        self.feature_transform = feature_transform
        if self.feature_transform:
            self.fstn = FeatureTransformNet() # Assuming FeatureTransformNet is defined elsewhere

    def forward(self, x):
        B, N, C = x.shape
        x = x.transpose(2, 1) # [B, C, N]

        x = F.relu(self.bn1(self.conv1(x)))
        trans_feat = None
        if self.feature_transform:
            trans_feat = self.fstn(x)
            x = torch.bmm(x.transpose(2, 1), trans_feat).transpose(2, 1)

        pointfeat = x
        x = F.relu(self.bn2(self.conv2(x)))
        x = self.bn3(self.conv3(x))
        x = torch.max(x, 2, keepdim=True)[0] # Global max pooling
        x = x.view(-1, 1024)

        if self.global_feat:
            return x, trans_feat
        else:
            x = x.view(-1, 1024, 1).repeat(1, 1, N)
            return torch.cat([x, pointfeat], 1), trans_feat

import torch.nn as nn
import torch.nn.functional as F
import torch

class PointNetLK(nn.Module):
    def __init__(self, feature_extractor, num_iterations=10, feature_dim=1024):
        super().__init__()
        self.feature_extractor = feature_extractor
        self.num_iterations = num_iterations
        self.update_net = nn.Sequential(
            nn.Linear(feature_dim * 2, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 6)
        )

    def forward(self, source, target):
        B = source.shape[0]
        T = torch.eye(4, device=source.device, dtype=source.dtype).unsqueeze(0).repeat(B, 1, 1)

        for i in range(self.num_iterations):
            source_transformed = torch.bmm(T[:, :3, :3], source.transpose(1, 2)).transpose(1, 2) + T[:, :3, 3].unsqueeze(1)

            f_s, _ = self.feature_extractor(source_transformed)
            f_t, _ = self.feature_extractor(target)

            feature_concat = torch.cat([f_s, f_t], dim=1)
            update_params = self.update_net(feature_concat)

            delta_t = update_params[:, :3]
            delta_r_params = update_params[:, 3:]

            angle = torch.norm(delta_r_params, dim=1, keepdim=True)
            axis = delta_r_params / (angle + 1e-8)

            K = torch.zeros(B, 3, 3, device=source.device, dtype=source.dtype)
            K[:, 0, 1] = -axis[:, 2]
            K[:, 0, 2] = axis[:, 1]
            K[:, 1, 0] = axis[:, 2]
            K[:, 1, 2] = -axis[:, 0]
            K[:, 2, 0] = -axis[:, 1]
            K[:, 2, 1] = axis[:, 0]

            I = torch.eye(3, device=source.device, dtype=source.dtype).unsqueeze(0).repeat(B, 1, 1)
            delta_R = I + torch.sin(angle).unsqueeze(-1) * K + (1 - torch.cos(angle).unsqueeze(-1)) * (K @ K)
            delta_T = torch.eye(4, device=source.device, dtype=source.dtype).unsqueeze(0).repeat(B, 1, 1)
            delta_T[:, :3, :3] = delta_R
            delta_T[:, :3, 3] = delta_t

            T = torch.bmm(delta_T, T)

        return T

"""**Section:** Utility Functions & Class Definitions
**Order:** 5 of 8 (PointNetLKUnifiedModel)
"""

import torch.nn as nn
import torch

class PointNetLKUnifiedModel(nn.Module):
    def __init__(self, num_iterations=10):
        super().__init__()
        self.feature_extractor = PointNetFeatureExtractor(global_feat=True, feature_transform=False) # Assuming PointNetFeatureExtractor is defined
        self.pointnetlk = PointNetLK(feature_extractor=self.feature_extractor, num_iterations=num_iterations) # Assuming PointNetLK is defined

    def forward(self, source, target, jaw_type):
        estimated_T = self.pointnetlk(source, target)
        return estimated_T


from torch.utils.data import Dataset
import os
import numpy as np
import torch
import open3d as o3d # Assuming open3d is installed

class UnlabeledPointCloudDataset(Dataset):
    def __init__(self, root_dir, n_points=2048):
        self.root_dir = root_dir
        self.case_ids = sorted(os.listdir(root_dir))
        self.n_points = n_points
        self.samples = []

        for case in self.case_ids:
            for part in ["lower", "upper"]:
                file_path = os.path.join(root_dir, case, f"{part}_raw.npy")
                if os.path.exists(file_path):
                    self.samples.append((case, part, file_path))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        case_id, part, file_path = self.samples[idx]
        jaw_type = 0 if part == "lower" else 1

        src = np.load(file_path)

        if src.shape[0] > 0:
            pcd = o3d.geometry.PointCloud()
            pcd.points = o3d.utility.Vector3dVector(src)
            pcd_cleaned, ind = pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)
            src = np.asarray(pcd_cleaned.points)

        if src.shape[0] > 0:
            mean = np.mean(src, axis=0)
            src = src - mean

            scale = np.max(np.abs(src))
            if scale > 1e-6:
                src = src / scale
            else:
                src = src

        if src.shape[0] > self.n_points:
            src = src[np.random.choice(src.shape[0], self.n_points, replace=False)]
        elif src.shape[0] < self.n_points:
             # Handle cases where there are fewer points than n_points by repeating points
             if src.shape[0] > 0:
                 repeat_indices = np.random.choice(src.shape[0], self.n_points - src.shape[0], replace=True)
                 src = np.vstack((src, src[repeat_indices]))
             else: # Handle empty point cloud after outlier removal
                  src = np.zeros((self.n_points, 3), dtype=np.float32)


        return {
            "case_id": case_id,
            "part": part,
            "source": torch.from_numpy(src).float(),
            "target": torch.from_numpy(src).float(),  # identity mapping for pretraining
            "jaw_type": torch.tensor(jaw_type, dtype=torch.long)
        }



from torch.utils.data import Dataset
import os
import numpy as np
import torch
import open3d as o3d # Assuming open3d is installed

class LabeledPointCloudDataset(Dataset):
    def __init__(self, ply_root, gt_root, n_points=2048):
        self.ply_root = ply_root
        self.gt_root = gt_root
        self.case_ids = sorted(os.listdir(ply_root))
        self.n_points = n_points
        self.samples = []

        for case in self.case_ids:
            upper_raw_path = os.path.join(ply_root, case, "upper_raw.ply")
            lower_raw_path = os.path.join(ply_root, case, "lower_raw.ply")
            upper_gt_path = os.path.join(gt_root, case, "upper_gt.npy")
            lower_gt_path = os.path.join(gt_root, case, "lower_gt.npy")

            if os.path.exists(upper_raw_path) and os.path.exists(upper_gt_path):
                self.samples.append((case, "upper", upper_raw_path, upper_gt_path))
            if os.path.exists(lower_raw_path) and os.path.exists(lower_gt_path):
                self.samples.append((case, "lower", lower_raw_path, lower_gt_path))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        case_id, part, ply_path, gt_path = self.samples[idx]
        jaw_type = 0 if part == "lower" else 1

        pcd = o3d.io.read_point_cloud(ply_path)
        src = np.asarray(pcd.points)

        gt_T = np.load(gt_path)

        src_h = np.hstack([src, np.ones((src.shape[0], 1))])
        tgt = (gt_T @ src_h.T).T[:, :3]

        if src.shape[0] > 0:
            src_pcd = o3d.geometry.PointCloud()
            src_pcd.points = o3d.utility.Vector3dVector(src)
            src_pcd_cleaned, ind_src = src_pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)
            src = np.asarray(src_pcd_cleaned.points)

            tgt_pcd = o3d.geometry.PointCloud()
            tgt_pcd.points = o3d.utility.Vector3dVector(tgt)
            # Apply the same outlier removal indices to the target point cloud
            # This ensures that corresponding points are removed from both
            if len(ind_src) > 0: # Check if any points were kept
                 tgt = tgt[ind_src]
            else: # Handle cases where all points were removed as outliers
                 src = np.array([]) # Set source to empty array as well
                 tgt = np.array([])


        if src.shape[0] > 0:
            mean_src = np.mean(src, axis=0)
            src = src - mean_src
            tgt = tgt - mean_src

            scale_src = np.max(np.abs(src))
            if scale_src > 1e-6:
                src = src / scale_src
                tgt = tgt / scale_src
            else:
                pass

        if src.shape[0] > self.n_points:
            indices = np.random.choice(src.shape[0], self.n_points, replace=False)
            src = src[indices]
            tgt = tgt[indices]
        elif src.shape[0] < self.n_points:
            # Handle cases where there are fewer points than n_points by repeating points
            if src.shape[0] > 0:
                repeat_indices = np.random.choice(src.shape[0], self.n_points - src.shape[0], replace=True)
                src = np.vstack((src, src[repeat_indices]))
                tgt = np.vstack((tgt, tgt[repeat_indices])) # Repeat corresponding target points
            else: # Handle empty point cloud after outlier removal
                 src = np.zeros((self.n_points, 3), dtype=np.float32)
                 tgt = np.zeros((self.n_points, 3), dtype=np.float32)


        return {
            "source": torch.from_numpy(src).float(),
            "target": torch.from_numpy(tgt).float(),
            "jaw_type": torch.tensor(jaw_type, dtype=torch.long),
            "gt_T": torch.from_numpy(gt_T).float()
        }



import open3d as o3d
import numpy as np
import nibabel as nib
from pathlib import Path
import torch
import torch.optim as optim
from torch.utils.data import DataLoader
import os
from tqdm import tqdm
import zipfile

# Utility function for visualizing registration results
def visualize_registration(patient_id, cbct_path, lower_stl_path, upper_stl_path, lower_transform=None, upper_transform=None, lower_transformed_pcd=None, upper_transformed_pcd=None):
    """
    Load original data, optionally apply transformation(s), and visualize.

    Args:
        patient_id (str): The ID of the patient.
        cbct_path (Path): Path to the CBCT NIfTI file.
        lower_stl_path (Path): Path to the lower jaw STL file.
        upper_stl_path (Path): Path to the upper jaw STL file.
        lower_transform (np.ndarray, optional): 4x4 transformation matrix for the lower jaw.
        upper_transform (np.ndarray, optional): 4x4 transformation matrix for the upper jaw.
        lower_transformed_pcd (o3d.geometry.PointCloud, optional): Pre-transformed lower jaw point cloud.
        upper_transformed_pcd (o3d.geometry.PointCloud, optional): Pre-transformed upper jaw point cloud.
    """
    print(f"--- Visualizing patient: {patient_id} ---")

    geometries_to_draw = []

    # --- Load and process CBCT data for visualization ---
    if cbct_path and cbct_path.exists():
        print("Loading CBCT data...")
        try:
            cbct_img = nib.load(str(cbct_path))
            cbct_data = cbct_img.get_fdata()

            # Extract point cloud from CBCT volume using thresholding
            threshold = 800  # Example Hounsfield Unit threshold - may need adjustment
            points = np.argwhere(cbct_data > threshold)

            # Get affine transformation matrix from NIfTI header
            affine = cbct_img.affine

            # Apply affine transformation to convert voxel indices to world coordinates
            # Points are (z, y, x) voxel indices from np.argwhere
            # Affine matrix transforms (x, y, z, 1)
            # We need to swap columns and add a homogeneous coordinate
            points_h = np.hstack([points[:, [2, 1, 0]], np.ones((points.shape[0], 1))]) # swap z, y, x to x, y, z and add 1
            points_world = (affine @ points_h.T).T[:, :3]

            cbct_pcd = o3d.geometry.PointCloud()
            cbct_pcd.points = o3d.utility.Vector3dVector(points_world)
            cbct_pcd.paint_uniform_color([0.5, 0.5, 0.5]) # Grey
            geometries_to_draw.append(cbct_pcd)
            print("CBCT point cloud loaded and transformed to world coordinates.")
        except Exception as e:
            print(f"❌ Error loading or processing CBCT data: {e}")
            cbct_pcd = None
    else:
        print("CBCT path not provided or does not exist. Skipping CBCT visualization.")
        cbct_pcd = None


    # --- Load and Visualize Original STL Meshes ---
    lower_mesh_orig = None
    if lower_stl_path and lower_stl_path.exists():
        print("Loading original lower STL...")
        try:
            lower_mesh_orig = o3d.io.read_triangle_mesh(str(lower_stl_path))
            lower_mesh_orig.paint_uniform_color([0.8, 0.8, 0.8]) # Light grey
            geometries_to_draw.append(lower_mesh_orig)
        except Exception as e:
            print(f"❌ Error loading original lower STL: {e}")

    upper_mesh_orig = None
    if upper_stl_path and upper_stl_path.exists():
        print("Loading original upper STL...")
        try:
            upper_mesh_orig = o3d.io.read_triangle_mesh(str(upper_stl_path))
            upper_mesh_orig.paint_uniform_color([0.8, 0.8, 0.8]) # Light grey
            geometries_to_draw.append(upper_mesh_orig)
        except Exception as e:
            print(f"❌ Error loading original upper STL: {e}")


    # --- Apply and Visualize Transformations (if provided) ---
    if lower_transform is not None and lower_mesh_orig is not None:
         print("Applying lower jaw transform...")
         lower_mesh_transformed = o3d.geometry.TriangleMesh(lower_mesh_orig)
         lower_mesh_transformed.transform(lower_transform)
         lower_mesh_transformed.paint_uniform_color([1, 0, 0])  # Red
         geometries_to_draw.append(lower_mesh_transformed)

    if upper_transform is not None and upper_mesh_orig is not None:
         print("Applying upper jaw transform...")
         upper_mesh_transformed = o3d.geometry.TriangleMesh(upper_mesh_orig)
         upper_mesh_transformed.transform(upper_transform)
         upper_mesh_transformed.paint_uniform_color([0, 1, 0])  # Green
         geometries_to_draw.append(upper_mesh_transformed)

    # --- Add pre-transformed point clouds (if provided) ---
    if lower_transformed_pcd is not None:
         print("Adding pre-transformed lower point cloud...")
         lower_transformed_pcd.paint_uniform_color([0, 0, 1]) # Blue
         geometries_to_draw.append(lower_transformed_pcd)

    if upper_transformed_pcd is not None:
         print("Adding pre-transformed upper point cloud...")
         upper_transformed_pcd.paint_uniform_color([1, 0.5, 0]) # Orange
         geometries_to_draw.append(upper_transformed_pcd)


    # --- Visualize ---
    if geometries_to_draw:
        print(f"Visualizing {len(geometries_to_draw)} geometries...")
        try:
             o3d.visualization.draw_geometries(geometries_to_draw)
        except Exception as e:
             print(f"❌ Error during Open3D visualization: {e}")
             print("This might be due to the environment not supporting graphical display.")
             print("To visualize in Colab, consider using compatible viewer or exporting results.")
    else:
        print("No geometries to draw.")

    print(f"--- Visualization complete for patient: {patient_id} ---")

# Utility function for post-processing registration results
def post_process_registration(source_np, target_np, initial_transform):
    """
    Applies statistical outlier removal and ICP registration for post-processing.

    Args:
        source_np (np.ndarray): Source point cloud as a numpy array.
        target_np (np.ndarray): Target point cloud as a numpy array.
        initial_transform (np.ndarray): Initial 4x4 transformation matrix (from NN).

    Returns:
        np.ndarray: Refined 4x4 transformation matrix after ICP.
    """
    # 1. Convert numpy arrays to Open3D point cloud objects
    source_pcd = o3d.geometry.PointCloud()
    source_pcd.points = o3d.utility.Vector3dVector(source_np)

    target_pcd = o3d.geometry.PointCloud()
    target_pcd.points = o3d.utility.Vector3dVector(target_np)

    # 2. Apply statistical outlier removal
    # Remove outliers from the source point cloud
    source_pcd_cleaned, ind = source_pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)
    # Remove outliers from the target point cloud
    target_pcd_cleaned, ind = target_pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)


    # 3. Implement ICP algorithm
    # Define convergence criteria
    threshold = 0.02 # Distance threshold for finding correspondences
    trans_init = initial_transform # Use the neural network output as initial transformation

    # Choose ICP variant (Point-to-point ICP)
    # Ensure point clouds are not empty after outlier removal before running ICP
    if len(source_pcd_cleaned.points) > 0 and len(target_pcd_cleaned.points) > 0:
        reg_p2p = o3d.registration.registration_icp(
            source_pcd_cleaned, target_pcd_cleaned, threshold, trans_init,
            o3d.registration.TransformationEstimationPointToPoint(),
            o3d.registration.ICPConvergenceCriteria(max_iteration=100))
        return reg_p2p.transformation
    else:
        print("Warning: One or both point clouds became empty after outlier removal. Skipping ICP.")
        return initial_transform # Return the initial transform if ICP cannot be performed


# Utility function to run training loop
def run_training(model, optimizer, data_loader, num_epochs, description):
    """
    Runs the training loop for a specified number of epochs.

    Args:
        model (torch.nn.Module): The model to train.
        optimizer (torch.optim.Optimizer): The optimizer.
        data_loader (torch.utils.data.DataLoader): The data loader.
        num_epochs (int): The number of epochs to run.
        description (str): A description for the training phase (e.g., "Pretrain", "Fine-Tune").
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device) # Ensure model is on the correct device

    print(f"Starting {description}...")

    for epoch in range(1, num_epochs + 1):
        model.train()
        total_loss = 0.0
        # Use tqdm for a progress bar
        for batch in tqdm(data_loader, desc=f"{description} Epoch {epoch:02d}"):
            src = batch['source'].to(device)
  
            tgt = batch['target'].to(device)
            jaw_type = batch['jaw_type'].to(device)

            # Pass source, target, and jaw_type through the model
            estimated_T = model(src, tgt, jaw_type)

            # Apply the estimated_T to the source point cloud to get the predicted point cloud
            src_h = torch.cat([src, torch.ones((src.shape[0], src.shape[1], 1), device=device)], dim=2)
            predicted_h = torch.bmm(estimated_T, src_h.transpose(1, 2)).transpose(1, 2)
            predicted = predicted_h[:, :, :3]

            # Calculate the Chamfer distance between the predicted and target point clouds
            loss = chamfer_distance(predicted, tgt).mean()

            # Perform the optimization step
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(data_loader)
        print(f"✅ {description} Epoch {epoch:02d} | Loss: {avg_loss:.6f}")

    print(f"Finished {description}.")


# Utility function to generate predictions
def generate_predictions(model, data_loader, output_dir, zip_filename="submission.zip"):
    """
    Generates predictions on the validation data and creates a submission zip file.

    Args:
        model (torch.nn.Module): The trained model.
        data_loader (torch.utils.data.DataLoader): The data loader for the prediction set.
        output_dir (str): The directory to save the predicted transformation matrices.
        zip_filename (str): The name for the output zip file.
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device) # Ensure model is on the correct device
    model.eval() # Set model to evaluation mode

    os.makedirs(output_dir, exist_ok=True)

    print("Starting prediction on validation data...")

    with torch.no_grad(): # Disable gradient calculation
        for batch in tqdm(data_loader, desc="Predicting Transformations"):
            src = batch['source'].to(device) # [B, N, 3]
   
            estimated_T = model(src, src, batch['jaw_type'].to(device)) # [B, 4, 4]

            # Save predicted transformation matrices
            T_np = estimated_T.cpu().numpy()

            case_ids = batch['case_id']
            parts = batch['part']

            for i in range(src.size(0)):
                case_id = case_ids[i]
                part = parts[i]

                case_output_dir = os.path.join(output_dir, case_id)
                os.makedirs(case_output_dir, exist_ok=True)

                # Determine the filename based on the part
                # The submission requires saving as *_gt.npy
                output_filename = os.path.join(case_output_dir, f"{part}_gt.npy")

                # Save the transformation matrix
                np.save(output_filename, T_np[i])

    print("✅ Predicted transformations saved.")

    # Create the submission zip file
    submission_zip_path = os.path.join(output_dir, zip_filename)
    with zipfile.ZipFile(submission_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for root, _, files in os.walk(output_dir):
            for file in files:
                file_path = os.path.join(root, file)
                # Get the relative path within the zip file (e.g., 002/upper_gt.npy)
                arcname = os.path.relpath(file_path, output_dir)
                zipf.write(file_path, arcname)

    print(f"✅ Submission zip file created at: {submission_zip_path}")


import os
import open3d as o3d
import numpy as np
from tqdm import tqdm

# Set root directory for all datasets
DATASET_ROOT = "/content/drive/MyDrive/DiceMed Pvt Ltd Company/STS_2025T2"

# Define a function to convert STL files to .npy point clouds
def convert_stls_to_npy(input_dir, output_dir, num_points=10000):
    """
    Converts STL files in the input directory to .npy point clouds
    and saves them in the output directory.

    Args:
        input_dir (str): Path to the root directory containing case subdirectories with STL files.
        output_dir (str): Path to the root directory where output .npy files will be saved.
        num_points (int): Number of points to sample from each STL mesh.
    """
    os.makedirs(output_dir, exist_ok=True)
    skipped_cases = []

    case_ids = sorted(os.listdir(input_dir))
    if not case_ids:
        print(f"Warning: No cases found in {input_dir}")
        return skipped_cases

    for case_id in tqdm(case_ids, desc=f"Converting {os.path.basename(input_dir)}"):
        case_input_path = os.path.join(input_dir, case_id)
        case_output_path = os.path.join(output_dir, case_id)

        if not os.path.isdir(case_input_path):
            continue

        try:
            upper_path = os.path.join(case_input_path, "upper.stl")
            lower_path = os.path.join(case_input_path, "lower.stl")

            # Check if both files exist
            if not os.path.exists(upper_path) and not os.path.exists(lower_path):
                skipped_cases.append((case_id, "missing_files"))
                continue

            # Load and sample upper mesh
            upper_points = None
            if os.path.exists(upper_path):
                upper_mesh = o3d.io.read_triangle_mesh(upper_path)
                if len(upper_mesh.triangles) > 0 and len(upper_mesh.vertices) > 0:
                    upper_pcd = upper_mesh.sample_points_uniformly(num_points)
                    upper_points = np.asarray(upper_pcd.points)
                else:
                     skipped_cases.append((case_id, "upper_empty"))
                     upper_points = np.array([]) # Handle empty mesh case

            # Load and sample lower mesh
            lower_points = None
            if os.path.exists(lower_path):
                lower_mesh = o3d.io.read_triangle_mesh(lower_path)
                if len(lower_mesh.triangles) > 0 and len(lower_mesh.vertices) > 0:
                     lower_pcd = lower_mesh.sample_points_uniformly(num_points)
                     lower_points = np.asarray(lower_pcd.points)
                else:
                     skipped_cases.append((case_id, "lower_empty"))
                     lower_points = np.array([]) # Handle empty mesh case


            # Save if points were successfully sampled (even if empty array due to empty mesh)
            if upper_points is not None or lower_points is not None:
                os.makedirs(case_output_path, exist_ok=True)
                if upper_points is not None:
                    np.save(os.path.join(case_output_path, "upper_raw.npy"), upper_points)
                if lower_points is not None:
                    np.save(os.path.join(case_output_path, "lower_raw.npy"), lower_points)
            else:
                skipped_cases.append((case_id, "sampling_failed"))


        except Exception as e:
            print(f"\n❌ Error in case {case_id}: {e}")
            skipped_cases.append((case_id, f"exception: {e}"))

    print(f"\nFinished converting {os.path.basename(input_dir)}. Skipped {len(skipped_cases)} cases.")
    return skipped_cases


NUM_POINTS = 10000 # Number of points to sample

# Convert Train-Unlabeled
train_unlabeled_input_dir = os.path.join(DATASET_ROOT, "Train-Unlabeled", "Images")
train_unlabeled_output_dir = os.path.join(DATASET_ROOT, "Unlabelled_Generated-NPY-STL")
print("Converting Train-Unlabeled dataset...")
skipped_unlabeled = convert_stls_to_npy(train_unlabeled_input_dir, train_unlabeled_output_dir, NUM_POINTS)

# Convert Validation
validation_input_dir = os.path.join(DATASET_ROOT, "Validation", "Images")
validation_output_dir = os.path.join(DATASET_ROOT, "Validation_Generated-NPY-STL")
print("\nConverting Validation dataset...")
skipped_validation = convert_stls_to_npy(validation_input_dir, validation_output_dir, NUM_POINTS)

# Convert Train-Labeled
train_labeled_input_dir = os.path.join(DATASET_ROOT, "Train-Labeled", "Images")
train_labeled_output_dir = os.path.join(DATASET_ROOT, "Train_Generated-NPY-STL")
print("\nConverting Train-Labeled dataset...")
skipped_labeled = convert_stls_to_npy(train_labeled_input_dir, train_labeled_output_dir, NUM_POINTS)

print("\n--- Conversion Summary ---")
print(f"Skipped Train-Unlabeled cases: {len(skipped_unlabeled)}")
for case_id, reason in skipped_unlabeled:
    print(f"  - {case_id}: {reason}")

print(f"Skipped Validation cases: {len(skipped_validation)}")
for case_id, reason in skipped_validation:
    print(f"  - {case_id}: {reason}")

print(f"Skipped Train-Labeled cases: {len(skipped_labeled)}")
for case_id, reason in skipped_labeled:
    print(f"  - {case_id}: {reason}")



import os
import numpy as np
import open3d as o3d
from tqdm import tqdm

# --- Paths ---
raw_npy_root = "/content/drive/MyDrive/DiceMed Pvt Ltd Company/STS_2025T2/Train_Generated-NPY-STL"
gt_root = "/content/drive/MyDrive/DiceMed Pvt Ltd Company/STS_2025T2/Train-Labeled/Labels"
output_root = "/content/drive/MyDrive/DiceMed Pvt Ltd Company/STS_2025T2/Aligned-Ply-STL"
os.makedirs(output_root, exist_ok=True)

# --- Transformation function ---
def apply_transform(points, T):
    points_h = np.hstack([points, np.ones((points.shape[0], 1))])  # → [N, 4]
    return (T @ points_h.T).T[:, :3]  # → [N, 3]

# --- Save as .ply ---
def save_ply(points, filename):
    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(points)
    o3d.io.write_point_cloud(filename, pcd)

# --- Process cases ---
for case_id in tqdm(sorted(os.listdir(raw_npy_root))):
    npy_path = os.path.join(raw_npy_root, case_id)
    label_path = os.path.join(gt_root, case_id)
    out_path = os.path.join(output_root, case_id)

    if not os.path.isdir(npy_path) or not os.path.isdir(label_path):
        continue

    try:
        # Load point clouds
        upper_raw = np.load(os.path.join(npy_path, "upper_raw.npy"))
        lower_raw = np.load(os.path.join(npy_path, "lower_raw.npy"))

        # Load GT transforms
        upper_T = np.load(os.path.join(label_path, "upper_gt.npy"))
        lower_T = np.load(os.path.join(label_path, "lower_gt.npy"))

        # Apply transformation
        upper_aligned = apply_transform(upper_raw, upper_T)
        lower_aligned = apply_transform(lower_raw, lower_T)

        # Save .ply
        os.makedirs(out_path, exist_ok=True)
        save_ply(upper_raw, os.path.join(out_path, "upper_raw.ply"))
        save_ply(upper_aligned, os.path.join(out_path, "upper_aligned.ply"))
        save_ply(lower_raw, os.path.join(out_path, "lower_raw.ply"))
        save_ply(lower_aligned, os.path.join(out_path, "lower_aligned.ply"))

    except Exception as e:
        print(f"❌ Error in case {case_id}: {e}")




device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

unlabeled_ds = UnlabeledPointCloudDataset(
    root_dir="/content/drive/MyDrive/DiceMed Pvt Ltd Company/STS_2025T2/Unlabelled_Generated-NPY-STL",
    n_points=2048
)
unlabeled_loader = DataLoader(unlabeled_ds, batch_size=8, shuffle=True, drop_last=True)

model = PointNetLKUnifiedModel(num_iterations=10).to(device)

optimizer = optim.Adam(model.parameters(), lr=1e-4)

num_epochs_pretrain = 200

run_training(model, optimizer, unlabeled_loader, num_epochs_pretrain, "PointNetLK Pretrain (Norm+Outlier)")



model_save_dir = os.path.join("/content/drive/MyDrive/DiceMed Pvt Ltd Company/DATASETS/MODELS")
os.makedirs(model_save_dir, exist_ok=True)

# Define the path to save the pretrained model state dict
pretrained_model_path = os.path.join(model_save_dir, "pointnetlk_unified_pretrained_norm_outlier_200epochs.pth")

# Save the model state dict
torch.save(model.state_dict(), pretrained_model_path)
print(f"✅ PointNetLK Unified Model pretrained (Norm+Outlier) state dict saved to: {pretrained_model_path}")



device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

labeled_ds = LabeledPointCloudDataset(
    ply_root="/content/drive/MyDrive/DiceMed Pvt Ltd Company/STS_2025T2/Aligned-Ply-STL",
    gt_root="/content/drive/MyDrive/DiceMed Pvt Ltd Company/STS_2025T2/Train-Labeled/Labels",
    n_points=2048
)
labeled_loader = DataLoader(labeled_ds, batch_size=8, shuffle=True, drop_last=True)

# Instantiate the model and load the pretrained weights
model = PointNetLKUnifiedModel(num_iterations=10).to(device)
# Assuming pretrained_model_path is defined or using the direct path
pretrained_model_path = os.path.join("/content/drive/MyDrive/DiceMed Pvt Ltd Company/DATASETS/MODELS", "pointnetlk_unified_pretrained_norm_outlier_200epochs.pth") # Updated filename
model.load_state_dict(torch.load(pretrained_model_path, map_location=device))

# Define optimizer for fine-tuning (using a lower learning rate)
optimizer = optim.Adam(model.parameters(), lr=1e-5)

num_epochs_finetune = 50 # Updated epoch number

run_training(model, optimizer, labeled_loader, num_epochs_finetune, "PointNetLK Fine-Tune (Norm+Outlier)")

# Define the path to save the fine-tuned model state dict
finetuned_model_path = os.path.join("/content/drive/MyDrive/DiceMed Pvt Ltd Company/DATASETS/MODELS", "pointnetlk_unified_finetuned_norm_outlier_200pretrain_50finetune.pth") # Updated filename

# Save the fine-tuned model state dict
torch.save(model.state_dict(), finetuned_model_path)
print(f"✅ PointNetLK Unified Model fine-tuned state dict saved to: {finetuned_model_path}")



import torch
from torch.utils.data import DataLoader
import os
import numpy as np
from tqdm import tqdm
import zipfile


prediction_output_root = os.path.join(DATASET_ROOT, "submission_predictions_FINAL_NormOutlier_200pretrain_50finetune") # Updated directory name
os.makedirs(prediction_output_root, exist_ok=True)

# Prepare the validation dataset using the UnlabeledPointCloudDataset
# This dataset provides the raw NPY files with preprocessing applied.
validation_ds = UnlabeledPointCloudDataset(
    root_dir=os.path.join(DATASET_ROOT, "Validation_Generated-NPY-STL"), # Path to the validation NPY files
    n_points=2048 # Use the same number of points as training
)
validation_loader = DataLoader(validation_ds, batch_size=8, shuffle=False, drop_last=False) # No need to shuffle or drop last for prediction

# Load the fine-tuned PointNetLK Unified Model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = PointNetLKUnifiedModel(num_iterations=10).to(device)
# Assuming finetuned_model_path is defined or using the direct path
finetuned_model_path = os.path.join("/content/drive/MyDrive/DiceMed Pvt Ltd Company/DATASETS/MODELS", "pointnetlk_unified_finetuned_norm_outlier_200pretrain_50finetune.pth") # Updated filename
model.load_state_dict(torch.load(finetuned_model_path, map_location=device))

# Generate predictions and the submission zip file
generate_predictions(model, validation_loader, prediction_output_root, zip_filename="Prediction_PointNetLK_NormOutlier_200pretrain_50finetune.zip") # Updated zip filename
